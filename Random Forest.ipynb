{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Improving on Decision Trees**\n",
    "\n",
    "The main drawback of decision trees are that they have a tendency to overfit. We mitigate this by improving their performance through pruning. We can also use decision trees to make a better model;\n",
    "\n",
    "Decision Trees are very susceptible to random idiosyncrasies in the training dataset. We say that Decision Trees have high variance since if you randomly change the training dataset, you may end up with a very different looking tree.\n",
    "\n",
    "One of the advantages of decision trees over a model like logistic regression is that they make no assumptions about how the data is structured. In logistic regression, we assume that we can draw a line to split the data. Sometimes our data just isn’t structured like that. A decision tree has the potential to get at the essence of the data no matter how it is structured.\n",
    "\n",
    "We will be learning about random forests in this module, which as you may guess from the name, is a model built with multiple trees. The goal of random forests is to take the advantages of decision trees while mitigating the variance issues.\n",
    "\n",
    "A random forest is an example of an ensemble because it uses multiple machine learning models to create a single model.\n",
    "\n",
    "**Bootstrapping**\n",
    "\n",
    "A bootstrapped sample is a random sample of datapoints where we randomly select with replacement datapoints from our original dataset to create a dataset of the same size. Randomly selecting with replacement means that we can choose the same datapoint multiple times. This means that in a bootstrapped sample, some datapoints from the original dataset will appear multiple times and some will not appear at all.\n",
    "\n",
    "For example if we have four datapoints A, B, C, D, these could be 3 resamples:\n",
    "\n",
    "    A, A, B, C\n",
    "    B, B, B, D\n",
    "    A, A, C, C\n",
    "\n",
    "We would rather be able to get more samples of data from the population, but as all we have is our training set, we use that to generate additional datasets.\n",
    "\n",
    "We use bootstrapping to mimic creating multiple samples.\n",
    "\n",
    "**Bagging Decision Trees**\n",
    "\n",
    "Bootstrap Aggregation (or Bagging) is a technique for reducing the variance in an individual model by creating an ensemble from multiple models built on bootstrapped samples.\n",
    "\n",
    "To bag decision trees, we create multiple (say 10) bootstrapped resamples of our training dataset. So if we have 100 datapoints in our training set, each of the resamples will have 100 datapoints randomly chosen from our training set. Recall that we randomly select with replacement, meaning that some datapoints will appear multiple times and some not at all.\n",
    "\n",
    "We create a decision tree with each of these 10 resamples.\n",
    "\n",
    "To make a prediction, we make a prediction with each of the 10 decision trees and then each decision tree gets a vote. The prediction with the most votes is the final prediction.\n",
    "\n",
    "When we bootstrap the training set, we're trying to wash out the variance of the decision tree. The average of several trees that have different training sets will create a model that more accurately gets at the essence of the data.\n",
    "\n",
    "**Decorrelate the Trees**\n",
    "\n",
    "With bagged decision trees, the trees may still be too similar to have fully created the ideal model. They are built on different resamples, but they all have access to the same features. Thus we will add some restrictions to the model when building each decision tree so the trees have more variation. We call this decorrelating the trees.\n",
    "\n",
    "When building a decision tree, at every node, we compare all the split thresholds for each feature to find the single best feature & split threshold. \n",
    "\n",
    "In a decision tree for a random forest, at each node, we randomly select a subset of the features to consider. This will result in us choosing a good, but not the best, feature to split on at each step. It’s important to note that the random selection of features happens at each node. So maybe at the first node we consider the Sex and Fare features and then at the second node, the Fare and Age features.\n",
    "\n",
    "A standard choice for the number of features to consider at each split is the square root of the number of features. So if we have 9 features, we will consider 3 of them at each node (randomly chosen).\n",
    "\n",
    "If we bag these decision trees, we get a random forest.\n",
    "\n",
    "Each decision tree within a random forest is probably worse than a standard decision tree. But when we average them we get a very strong model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimensions (569, 30)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer_data = load_breast_cancer()\n",
    "df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])\n",
    "df['target'] = cancer_data['target']\n",
    "\n",
    "X = df[cancer_data.feature_names].values\n",
    "y = df['target'].values\n",
    "print('data dimensions', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 101)\n",
    "\n",
    "rf= RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: [1]\n",
      "true value: 1\n"
     ]
    }
   ],
   "source": [
    "first_row = X_test[0]\n",
    "\n",
    "print(\"prediction:\", rf.predict([first_row]))\n",
    "print(\"true value:\", y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the model to make a prediction. \n",
    "\n",
    "     We take the first row of the test set and see what the prediction is. \n",
    "     The predict method takes an 2D array of points, so even when we have just one point, we have to put it in a list\n",
    "     These results mean that the model predicted that the lump was cancerous and that was correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.965034965034965\n"
     ]
    }
   ],
   "source": [
    "#using the score method to calculate the accuracy over the whole test set.\n",
    "\n",
    "print(\"Random Forest Accuracy:\", rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning a Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Parameters**\n",
    "\n",
    "Since a random forest is made up of decision trees, we have all the same tuning parameters for prepruning as we did for decision trees: max_depth, min_samples_leaf, and max_leaf_nodes.\n",
    "\n",
    "With random forests, it is generally not important to tune these as overfitting is generally not an issue.\n",
    "\n",
    "\n",
    "New tuning parameters: \n",
    "\n",
    "      n_estimators (the number of trees) and\n",
    "      max_features (the number of features to consider at each split).\n",
    "\n",
    "The default for the max features is the square root of p, where p is the number of features (or predictors). The default is generally a good choice for max features and we usually will not need to change it, but you can set it to a fixed number with the following code.\n",
    "  \n",
    "      rf = RandomForestClassifier(max_features=5)\n",
    "      \n",
    "The default number of estimators (decision trees) is 100. This often works well but may in some cases be too small. You can set it to another number as follows\n",
    "\n",
    "     rf = RandomForestClassifier(n_estimators=15)\n",
    "     \n",
    "     \n",
    "**Grid Search**\n",
    "\n",
    "As is in the Decision Tree module, scikit-learn has built in a Grid Search class to help us find the optimal choice of parameters.\n",
    "\n",
    "We need to define the parameter grid of the parameters we want to vary and give a list of the values to try.\n",
    "\n",
    "     param_grid = {\n",
    "    'n_estimators': [10, 25, 50, 75, 100],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'n_estimators': 25}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_estimators': [10, 25, 50, 75, 100]}\n",
    "\n",
    "rf1 = RandomForestClassifier(random_state = 123)\n",
    "gs = GridSearchCV(rf1, param_grid, scoring = 'f1', cv = 5)\n",
    "\n",
    "gs.fit(X, y)\n",
    "print(\"best params:\", gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we've used the fit method to run the grid search. The best parameters will be stored in the best_params_ attribute.\n",
    "\n",
    "On defaultthe parameters returned are those that yield the highest accuracy as that is the default metric. \n",
    "\n",
    "You may get slightly different results each time you run this as the random split in the 5 folds may affect which has the best accuracy score. To avoid outputting a different best parameter each time, we set the random_state in the classifier.\n",
    "\n",
    "Accuracy will work okay for us in this case as the classes in the breast cancer dataset are reasonably balanced. If the classes are imbalanced, we would want to use an alternative metric, like the f1-score (as we've done), by changing scoring(metric) parameter to \"f1\" \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elbow Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a parameter like the number of trees in a random forest, increasing the number of trees will never hurt performance. Increasing the number trees will increase performance until a point where it levels out.\n",
    "\n",
    "The more trees, however, the more complicated the algorithm. A more complicated algorithm is more resource intensive to use. Generally it is worth adding complexity to the model if it improves performance but we do not want to unnecessarily add complexity.\n",
    "\n",
    "We use an **Elbow Graph** to find the sweet spot. Elbow Graph is a model that optimizes performance without adding unnecessary complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=123),\n",
       "             param_grid={'n_estimators': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n",
       "                                          13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
       "                                          22, 23, 24, 25, 26, 27, 28, 29, 30, ...]})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators = list(range(1, 101))\n",
    "\n",
    "param_grid = {'n_estimators': n_estimators}\n",
    "\n",
    "rf2 = RandomForestClassifier(random_state = 123)\n",
    "gs2 = GridSearchCV(rf2, param_grid, cv = 5)\n",
    "gs2.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the optimal value, we do a Grid Search trying all the values from 1 to 100 for n_estimators.\n",
    "\n",
    "Instead of just looking at the best params, we are going to use the entire result from the grid search. \n",
    "\n",
    "The values are located in the cv_results_ attribute. This is a dictionary with a lot of data, however, we will only need one of the keys: mean_test_score. We pull out these values and store them as a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90506133, 0.91386431, 0.94027325, 0.94199658, 0.9561093 ,\n",
       "       0.95081509, 0.95961807, 0.95609377, 0.95960255, 0.95960255,\n",
       "       0.96311132, 0.9648657 , 0.96489676, 0.96314237, 0.9719143 ,\n",
       "       0.96488123, 0.97015991, 0.96840553, 0.97015991, 0.96840553,\n",
       "       0.96840553, 0.96840553, 0.97015991, 0.96840553, 0.97015991,\n",
       "       0.9719143 , 0.97014439, 0.97014439, 0.96839   , 0.97365316,\n",
       "       0.97189877, 0.96662009, 0.97014439, 0.9648657 , 0.9648657 ,\n",
       "       0.96309579, 0.96662009, 0.96835895, 0.96660456, 0.96485018,\n",
       "       0.9648657 , 0.96662009, 0.96662009, 0.96309579, 0.96309579,\n",
       "       0.96309579, 0.9648657 , 0.96311132, 0.9648657 , 0.9648657 ,\n",
       "       0.9648657 , 0.96485018, 0.96309579, 0.96309579, 0.96309579,\n",
       "       0.96309579, 0.96309579, 0.96309579, 0.96309579, 0.96309579,\n",
       "       0.96309579, 0.96309579, 0.96309579, 0.96309579, 0.96134141,\n",
       "       0.95958702, 0.96134141, 0.95958702, 0.96134141, 0.95958702,\n",
       "       0.96134141, 0.96309579, 0.96311132, 0.96134141, 0.96488123,\n",
       "       0.96311132, 0.96311132, 0.9648657 , 0.96663562, 0.96662009,\n",
       "       0.9648657 , 0.9648657 , 0.9648657 , 0.96662009, 0.96663562,\n",
       "       0.96663562, 0.96488123, 0.9648657 , 0.96488123, 0.9648657 ,\n",
       "       0.96488123, 0.96663562, 0.96488123, 0.96312684, 0.96312684,\n",
       "       0.96312684, 0.96312684, 0.96312684, 0.96488123, 0.96312684])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = gs2.cv_results_['mean_test_score']\n",
    "scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We the use matplotlib to plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEXCAYAAACH/8KRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAw8UlEQVR4nO3dd5xU9b3/8ddnG0vvvS0iUlRQWVDR2GNssSUxGhURLMRgNNfcJJrmvbm58f6Sa2JujFhAxILdaIxEDaLYpYPSpIOwdHaXsv3z++Ochdlllh2Ynd1l5v18PObBnP6Z82DnM996zN0RERE5XGkNHYCIiBzZlEhERCQuSiQiIhIXJRIREYmLEomIiMRFiUREROKiRCIpxcxGmdkHEctuZkc3ZEz1IVU+pzQMJRJJOma22sz2mtmuiNdfGjquSmbW1cweNbMNYWwrzWySmQ1o6NhEDocSiSSrb7p7i4jXuIYOCMDM2gMfAc2ArwEtgZOA94Cv13BMRr0FKHIYlEhE4KKwVLDVzH5vZmkAZpZmZr8wszVmttnMJptZ63DbE2Z2V/i+e1h1dFu4fLSZbTczi3KtHwEFwPXuvsIDO939cXf/v/D4nPB8Y8xsLfBOuP4FM8szs3wzm2Fmx1aeNCzRjDezt82s0MzeM7Pe1a59npl9aWY7zOzBGuITOWRKJCJwBZBLUDK4DBgdrh8Vvs4GjgJaAJVVZO8BZ4XvzwRWhv8CnAG879HnHzoPeMXdK2KI60xgIPCNcHkq0A/oBMwBnq62/7XAb4AOwLwo2y8BhgFDgKsizisSFyUSSVZ/M7OdEa+bD7Lv/7j7dndfC/wJuCZcfy1wv7uvdPddwN3A1WFV03vA18LSyxnA/wNOC487M9weTQcgr3LBzC4N4ys0s7eq7Xuvu+92970A7j7R3QvdvRi4FxhSWUIK/cPdZ4Tbfw6camY9I7bfF5Z+1gLTgRMOck9EYqZEIsnqcndvE/F69CD7rot4vwboFr7vFi5HbssAOrv7CmAXwZfx14DXgQ1m1p+DJ5JtQNfKBXd/zd3bEFR5ZdUUl5mlm9l9ZrbCzAqA1eGmDtH2DxPf9ojPAhEJDNhDUMISiZsSiQhE/mrvBWwI328AelfbVgZsCpffA74NZLn7V+HySKAtQdVSNNOAyyvbYWoRWTX2PYJqt/OA1kBOuD6ynWPf5zCzFkC7iM8ikjBKJCLw72bWNqwGugN4Llw/BfiRmfUJv5j/G3jO3cvC7e8B44AZ4fK7wO3AB+5eXsO17idINE+aWV8LtKT2aqaWQDFBiaZZGEt1F5nZ6WaWRdBW8qm7r4uyn0idUiKRZPX3auNIXjnIvq8CswlKEf8AJoTrJwJPEiSKVUARQaKo9B7BF3xlIvmA4Et+BjVw963AKeG5PgAKw+u2BL5/kBgnE1StfQUsAj6Jss8zwK8JqrSGErTxiCSc6cFWIkc+M5sErHf3XzR0LJJ6VCIREZG4JCyRmNnEcBDX5zVsNzP7s5ktN7MFZnZSxLYLzGxpuO1niYpRRETil7CqLTM7g6B75GR3Py7K9osI6psvAk4GHnD3k80sHVhGMF3EemAmcI27L0pIoCIiEpeElUjcfQZBo19NLiNIMu7unwBtzKwrMBxYHg4CKwGeDfcVEZFGqCEng+tO1YFg68N10dafXNNJzOwW4BaA5s2bDx0wQBOoiojEavbs2VvdvWM852jIRBJtwjg/yPqo3P0R4BGA3NxcnzVrVt1EJyKSAsxsTe17HVxDJpL1VB1R3INgFG5WDetFRKQRasjuv68BI8PeW6cA+e6+kaBxvV84mjgLuDrcV0REGqGElUjMbArBNNsdzGw9wYjbTAB3Hw+8QdBjaznBBHI3htvKzGwc8CaQDkx09y8SFaeIiMQnYYnE3a+pZbsDP6hh2xsEiUZERBo5jWwXEZG4KJGIiEhclEhERCQuSiQiIhIXJRIREYmLEomIiMRFiUREROKiRCIiInFRIhERkbgokYiISFyUSEREJC5KJCIiEhclEhERiYsSiYiIxEWJRERE4qJEIiIicVEiERGRuCiRiIhIXJRIREQkLkokIiISFyUSERGJixKJiIjERYlERETiokQiIiJxUSIREZG4KJGIiEhclEhERCQuSiQiIhIXJRIREYmLEomIiMRFiUREROKiRCIiInFRIhERkbgokYiISFyUSEREJC4JTSRmdoGZLTWz5Wb2syjb25rZK2a2wMw+M7PjIrb9yMy+MLPPzWyKmWUnMlYRETk8CUskZpYOPAhcCAwCrjGzQdV2uweY5+6DgZHAA+Gx3YEfArnufhyQDlydqFhFROTwJbJEMhxY7u4r3b0EeBa4rNo+g4BpAO6+BMgxs87htgygqZllAM2ADQmMVUREDlMiE0l3YF3E8vpwXaT5wJUAZjYc6A30cPevgD8Aa4GNQL67vxXtImZ2i5nNMrNZW7ZsqeOPICIitUlkIrEo67za8n1AWzObB9wOzAXKzKwtQemlD9ANaG5m10W7iLs/4u657p7bsWPHOgteRERik5HAc68HekYs96Ba9ZS7FwA3ApiZAavC1zeAVe6+Jdz2MjACeCqB8YqIyGFIZIlkJtDPzPqYWRZBY/lrkTuYWZtwG8BNwIwwuawFTjGzZmGCORdYnMBYRUTkMCWsROLuZWY2DniToNfVRHf/wszGhtvHAwOByWZWDiwCxoTbPjWzF4E5QBlBldcjiYpVREQOn7lXb7Y4cuXm5vqsWbMaOgwRkSOGmc1299x4zqGR7SIiEhclEhERiYsSiYiIxEWJROK3YgXcfhu0bw3pacG/t98WrBeRpKdEIvGZOhWGDoEFk+HaCvh5i+DfBZOD9VOnNnSEIpJgiRyQKMluxQq45jvwLYeeEf+V2hmcDRxdFmyfPR/69m2wMEUksVQikcP3p/+FIVRNIpF6ZgTbH/hjfUYlIvVMiUQO3zNPw5BoU6pFGGLwjGa2EUlmSiRy+HYWQptaEklrw3cU1k88ItIglEjk8LVpCTtrmRkh3ylu0YxkmkFBRKpSImlkdu4poaSsoqHDqGLrruLoieB718L8gyeI8rkVPHfMmTz2/qoERSciDU2JpBGpqHC+/scZ/OWdLxs6lH1enfcVw377L37zepTJl++8K3g02bqy6AevKyNtobHimpv576mLWbd9T0JjFZGGoUTSiKzbsYcthcV8uGJbQ4cCwIxlW7jr+fm0a5bFxA9XMfnj1VV36NsXprwALxlML4ftFVDusL2C8nfK4CXDprzAjSPPwx2mL93cIJ9DRBJLiaQRWZIXNEov/Cq/wau35q/bydinZtOvc0veuesszhvYmXtf+4LpS6olgwsvDMaJDBkFz6Tj/72LwokVFA+6Plh/4YX06dCcnPbNDjxWRJKCEkkjsixMJCVlFSzeWFBlW2l5Ba/MXc+ekgOrkVZs2cXUhRsP6VolZRU8N3Mt+XtKD9i2autubpw0k3bNs3jixmG0bpbJA1efwMCurRj3zBxem7+Bd5du5t2lm4M4+/aFP/8Ftu7kqgc/4Jrf/ZNm48dXGYR4Vv9OfLRiG3tLyqtca/vuEjYVFB1S7CLSuCiRNCJLNhXSMjsY3Ddn7Y4q216bt4EfPTefO56dR3nF/gbur3bu5epHPuH7T8/h2c/WxnQdd+fulxfy05cWcvPkWRSX7f9y3767hBsf/wyAyaOH06lVNgDNm2QwcdQwWjXN5IdT5jLq8ZmMenwmFz7wPi/NXg8EHQVmr9nB2f07HXDNcwZ0orisgk9WVq22u3nyLK577NOY4haRxkmJpBFZmlfIyX3a07V1NnPX7qyybfrSzWSmG28v2sR9U4OG78KiUsZMmklRSTnDctryi799zofLt9Z6nQenL+elOes5Z0AnPlu9nbtfWoi7U1xWzq1PzmJDfhGPjhzKUR1bVDmuc6ts3vzRGbx82whevm0EL31/BKcd3Z6fvLSA6Us3M+PLrVR4UPqobnifdjTNTK/STjJn7Q5mr9nBl5t3sXLLrsO4YyLSGGiurUaiuKycVVt3c8GxXcjKMOau218iKSuvYMayLVx+QneaZaXz6Pur6NmuGdMWb+bLzbuYdOMwTujZhm8/9DFjn5rNy98fQb/OLaNe5+/zN/CHt5ZxxYnduf+qIfzfO8u5/+1l9G7fnNXbdjNz9Q7+fM2JDO3dLurxrbIzOalX233L468bytWPfMJtT82hf5eWtG2WyQk92xxwXHZmOqcd3YF3lmzmPy51zIyJH6yiaWY6e0vLeWfJ5gMSl4gcGZRIGokVm3dTXuH079KS1k0zeWNhHlsKi+nYsglz1u6koKiMswd04vxBnVm7fQ+/evULAH535fF8rV9HACaMyuXyBz9i1OMzOXtAxwOuUeHw4uz1DM9px33fOh4z4/Zzjmb1tt388V/LAPjx+cdw6ZBuMcfdMjuTSTcO59vjP2Leup1cfkI30tOij3Y/e0BH/rV4Eyu27KJpVgZTP89jzOl9eHfpZt5ZspmbvnbUod423J2nPlnDCT3bcnyP1od8vIjET4mkkVi6KWhc79+lJd3aBO0Sc9fu4PxjuzB96WYy0ozT+3UgIz2N//veSXz/qdkMy2nHNcN77TtHj7bNmHBDLnc+N4+pC/OiXmdw99aMv34oTTLSATAzfnfl8ewtKadL62x+cPbRhxx7x5ZNmDx6OP/2/Hy+d3LvGverrPJ6Z8lmtu0qwd0ZeWpv0sx47P2VFBSV0io785Cu/dd3V/D7N5fSumkmr9w2QqUakQagRNJILMkrJDPd6NOhOeUVTma6MXfdziCRLNlMbk7bfV+yLZpk8OSYk6OeZ0jPNkz/8VmHdO0mGek8dN3QuOLv3b45L31/xEH36d6mKQO6tOQfC/NYtWUXFx7XlR5tm3HuwE6Mf28F7y/bysWDu8Z8zb/P38Dv31zKeQM7M2ftDkZPmsnLt51Gu+ZZcX0WETk0amxvJJbmFdK3Ywsy09PIzkxnUNdWzF27gw0797IkrzBqT6gj0Vn9OzF/XVBVN/r0PgCc2LMNbZpl8s4hjDOZvWYHd70wn2E5bXnw2hN5dORQNuQXceuTVXuhiUjiqUTSSCzLK2R4n/0N3Cf2astzM9cxbfEmIOg+mwzOGRCUPob0bMNJvdoAkJGexpnHdOTdpZspr/Aa21gq5eUXccvkWXRtnc3D1+fSJCOdob3b8YfvDOGHU+by0xcXcP9VJ5B2kPO8/+UWnv1sHRXhHGJNMtK45+KBdGqZXWefVRqP1+ZvqDLWqk2zLH7yjf60jbH0Wl7hPPCvZXy5OXrvwuZNMrjr/GPo2rpp3LHu3FPCn6ct59pTetH3CKmqVSJpBPL3lrIhv4hjuuzvaXVirzZM+mg1j76/iu5tmnJ0pyPjP1RtTurVhguO7cLIU3tjtv+L/pwBnXh13gbmr99ZpVdYNI++v5L8vaU8d+upVaqxLh3SjbXbdvOHt5bRuVU2d180MOrxn67cxpgnZtEqO5N2zYPqwuWbd9GpVTb31HCMHLn+Pn8Ddzw7ly6tsveN01q1dTdL8wp4+qZTaJqVftDj3Z1fvvo5z3y6lr4dm0f9obN2+x7mr9vJC2NPpU2zw69a3VtSzpgnZjF7zQ7++flGXrptRJ0kp0RTImkElm0KRrQPiEgklV+ma7fv4fpTqn7pHsky0tMYf/2B7TFnHtOR9DTjncWbD5pICotKeW7mOi4e3DVqcv3B2UezqaCYh2espEOLJtx8RtWeYEvyCrhp8ix6tm3Ki2NH7PtFOu6ZOUz5bC13nNuP5k30Z5EsPvhyK//2/DyG9W7H5DHDyc4MksbUhRu57Zk5jHtmDg9fP5SM9Jpr+R+Y9iXPfLqWsWf25WcXDoi6z0crtjJq4kxuemIWT445udbkFE1ZeQXjnpnDnLU7+PH5xzD+vZXcMPEzXrh1BK2bHVonlPqmNpJGoHKOrf5dWu1b16NtUzq0CL7konXlTTZtmmUxtHdbptXSTvL8rPXsKi5jTNi+Up2Zce+lx3LR8V347RuLeWXu+n3b1m3fw8gJn9E8K4PJY06uUq0x+vQ+FBaV8eLs9dFOK0egz7/K59YnZ9G3YwsevSF3XxIBuPD4rvzmsuOYtmQzd7+8sMbn5Tz1yRr+9K8v+dZJPfjpBf1rvNaIvh3409UnMHvtDm6fMoey8kObK69ytolpSzbzm8uOY9w5/Xjk+qGs3rqHmybPpKi0cbf7WTI9cCg3N9dnzZrV0GEcYGP+Xq6f8Blrtu3et+6kXm2ZOGoYzZtk8Mu/fc7f5n3Fgl+fX6XkcfPkWby3bAvzf3X+Yf3COdKMf28F901dQmZ6cA/SzPjhuf32dUkur3DO+sN0OrfM5sVaeogVl5UzauJMPl65bd/5yiucFk0yeGHsCPp3OXDA5hV//ZAdu0t4566zDtq+AkEVxM2TZ9G5VTa///bgKvsv3ljA6Ekz2bqruNbPnJmexs8vHsi1B+k2Lfvl7ynl6kc/4bpTeh30nq3bvocr/vohTTLSefm2EXRuFb3t649vL+OBaV+SkWZEK/SXljvnDOjEw9cPJfMgpZZKT36yhl/+7XPS04xa/gtV4Q5lFc4Pz+3Hv339mH3rX1+wgdunzOXrAzvz0HVDa20/rE1ZeQV3PDePvSXlPHTdSTTJSMfMZrt7bjznVRk+wXYVlzF60izy8osYc/pRpBkUlVYw6aNV3PHsXB6+PpeleYX079zygOqrf/9Gf64e1jMlkgjANcN6UVRavm/m40UbC/j9m0vp0iqbbw3twduLNrFu+17uubD2dowmGek8MnIokz9ew+7iYKJLM7j4+G5RkwjA6NP6cPuUubyzZDPnDepc47krKpx/e34eH4TT0XRomcXdYUybCooYPWkm7nBzDAMsZ6/ZwS//9jldW2dzzoCarymB52atZfHGgoPes/y9pYyeNJOSsgqeveXUGpMIwJ3n9aN7m6asjviRF6lV00xuODUnpiQCcP0pvWnbLJNFGwpq37ma3u2bcVVuzyrrLhncjc0Fxfzn64u4b+pifn7xoEM+byV3596/f8E/FgSdDu5+aSH/e9WQwz5fJCWSBCorr+CHU+aybFMhE0cN48xj9ldR9enQjF+++gW/eX0RS/IKuCTKaPJjOrfkmBqmOklGrZtlcud5+3+NlZZXcMPEz/jZywvo3rYpEz9cRY+2TTn/2C4xna9lduYhDbC88LgudGudzYQPVh00kfzPm0uY+nkev7h4IKu37ebh91aS0745l53QjTFPzCR/bykvjD2VY7vVPtJ+T0kZVz38Mbc/M5cXxo5gULdWtR6TqsornMkfr+GkXm0oLXfGPTP3gPtcWl7BD56ew6qtu5k8ZnitnVTMjKuG9TzoPofqksHduGRw7LND1Gb06X1Ys203j76/ipwOzQ+79Drhg1U89clabj3jKJo3yeD+t5eR06F5ncSoRJJA//WPxbyzZDP/dflxVZIIwPWn5rBq6x4mfhg8gnZADb+SU1lmehoPXTuUKx/6kJuemMWu4jJ+cfHAuIv3NclIT2PkiBzum7qE2Wt2RP0S+vv8DTz83kquO6UXY07vQ3mFs277Xn7xt895ftY6Fm0o4LEbcmNKIgDNsjKYcMMwLn/wQ0ZPmsmzt5wSc5fU+mDGIc82cLiKy8opKt3fttAqO6NKKX3a4k2s37GXn180kJN6t+XyBz9kzKRZTLnllH299+6bupgPlm/l998ezIi+Heol7vrwy0sG7ZsaqUurbHJzos+FV5MPl2/lt28s5oJju/DTCwZgBqu37eb+t5fVSXxqI0mQf36ex9inZnPT6X34xSXRi6PlFc6tT87mX4s38fytp1YZRyL7rd22h8v/+iHFpeV8fM+5Cf1iy99Tyim/m8begzRunnlMRybckLuvp09hUSnfGf8xS/IKufebgxh1WvSOAAezaEMB3xn/EbtLGl+j6uUndKt1XE68Zq/ZzqiJMyks3v+8nZP7tOOJ0ft7Wn3v0U9YvXU3M35yNhnpaSzeWMC3Hzrwnv3g7L78+zei9646ku0qLuM74z8+4FlFsRrSozXP3nLqvqry4rJyrp/wGS+MHRF3G4kSSYJc8dcP2R423B7sF/TeknL+tXgTFx/fNaF/qEe6Ndt2s3NPKUOizCxc1z5esY1FNfyxNs1M57ITuh3QRXjrrmIWrN8ZVzvH4o0FfNRIHrNcadXWXTz1yVq+f1ZffnpBYr6c12zbzRV//YjWTTO57pSg2mb77mIenL6CSwZ35c9Xn8jyLbs4/48z+MkF/bntrP3VldXvWYcWWXxzcLek/VvatquY1xdspKzi0L63s9KNSwZ3O6C0u2N3Ce1aNFFje2M0Z+0O5q7dyb3fHFRrNUzTrHS+eQiz7aaq3u2b07t9/Vzr1L7tObXvoV2sQ4smcTeWD+zaioFdG1cbibtT4fDQuyvo0755nbcn5O8p5cZJM6lwZ+KoYfSJqLNvmZ3JfVOXkNO+OTv2lNAkI42rh/WqcnxjvGeJ1L5FE24YkVNn56uralQlkgSY+MEqWmZn8J3cuv2jE6lvZsZ/XHos67bv4Z5XFtK9bVNOO/rAtoe9JeVkZaRF/eG0q7iMFlEGeZaUVTD2qdms276Hp8acXCWJANx6xlGs3rqbv0xfTkaaceVJ3TUhZyOV0AGJZnaBmS01s+Vm9rMo29ua2StmtsDMPjOz4yK2tTGzF81siZktNrNTExlrXflq516mfp7HNcN7aYS0JIXM9DQevPYkjurYnLFPzWZJXtVqv7Xb9nDm76dz1cMfs7dae8Vr8zcw+N43D2jUrahwfvrSAj5euY3/9+3BnHzUgSVAM+M3lx/H6Ud3oKzC6/SXuNSthCUSM0sHHgQuBAYB15hZ9Vbne4B57j4YGAk8ELHtAeCf7j4AGAIsTlSsdWnyx6v3PWdDJFm0ys7k8RuH0zwrg5ETPmPd9j1A0DY0cuKn7C0pZ87aHYx7Zv+o7ve/3MJdz8+jZXYmf572JU9+vHrf+e775xJemfsVd339GK44sUeN181MT+PRkbm8fvvpMfeEk/qXyBLJcGC5u6909xLgWeCyavsMAqYBuPsSIMfMOptZK+AMYEK4rcTddyYw1jqxu7iMKZ+u3fecDZFk0r1NUyaPGU5RaTk3TAySyehJM8krKGLS6OFVphxZsH4nY5+cTd+OLXj3x2dx3sBO/Oq1L3hj4UYembGCR2asZOSpvRl3Tu3jfJpmpXNcdyWRxiyRdS/dgXURy+uB6k9jmg9cCXxgZsOB3kAPoBzYAjxuZkOA2cAd7n7A8FMzuwW4BaBXr17VN9eJxRsLKK/wqP+Z/z5/A3n5RUAwZ1bwnI2chMQh0tCO6dySCaOGcd1jn3Lu/75HuTuPXD+Uob3bMrR3W7YUFvPAtC95dd4GOrZswhOjh9O2eRb/d81JXDfhU+54di6l5c7Fx3fl1988NmkmI011iSyRRPsfUr3P2n1AWzObB9wOzAXKCBLcScBD7n4isBs4oI0FwN0fcfdcd8/t2DExkxve/fLCfVMuRJq/bie3T5nLb99YzG/fWMxLc9ZzylHtap0GXeRINiynHX/53klkZ6Zx35XHc+7A/b3V7jyvH6NG5NCueRaTxwzfNz1J06x0JtyQyzGdW3LmMR25/7tDEjawVOpfTCUSM3sJmAhMdfdYp7VcD0R2W+oBbIjcwd0LgBvDaxiwKnw1A9a7+6fhri9SQyJJtIoKZ2leIXtLy/nHwg1V6nMnfriKFk0yeOeuM2kWNqw3y0zXryxJel8f1Jl5vzr/gPEalbMv/+qSQQdsa9Msi7+POx0z9DeSZGItkTwEfA/40szuM7NYRibNBPqZWR8zywKuBl6L3CHsmVXZn+8mYIa7F7h7HrDOzCrnbT4XWBRjrHVq3Y49+0Y5T/hg1b7ppvPyi/jHgo1clduTTq2yadEkgxZNMpJ2IJRIdQf7v17TtrQ0UxJJQjElEnf/l7tfS1DdtBp428w+MrMbzSzqfBXuXgaMA94k6HH1vLt/YWZjzWxsuNtA4AszW0LQu+uOiFPcDjxtZguAE4D/PuRPVwcqnxVyVW4PPv+qgJmrdwBB76wKd248LachwhIRaTRibmw3s/bAdcD1BG0ZTwOnAzcAZ0U7xt3fAN6otm58xPuPgX41HDsPiGvYfl1YGiaSn14wgLcWbWLCBys5vntrnvlsLV8f1Jme7dQ7S0RSW0wlEjN7GXifoO3im+5+qbs/5+63A8nxMPEaLM0rpFe7ZrRv0YTvDe/FW4s28adpy9i5p5Qxp9f+vAkRkWQXaxvJX9x9kLv/zt03Rm6Id7Kvxm5JXsG+ByGNPDWHdDMefi8olQzLUe8sEZFYE8lAM2tTuRBObXJbYkJqGHPW7uCWybMoLts/xUNRaTmrt+3Z96yQLq2zuXhwVwBGn56jRkMREWJPJDdHjix39x3AzQmJqIG8u3QLby3axMxVO/atW7FlF+UVXuUphXd9vT+3nHEUFx+vGXtFRCD2RJJmET+/w3m0kmoazrz8vQC8s2TzvnWVDe2RTy/s1b4Z91w0kKyMhM53KSJyxIj12/BN4HkzO9fMzgGmAP9MXFj1b2M4zcm7S6smkqz0tDp7rrGISDKKtfvvT4Fbge8TTH3yFvBYooJqCHn5RaQZrNy6m9Vbd5PToTlL8grp26kFmekqfYiI1CTWAYkV7v6Qu3/b3b/l7g+7e+N7uHQc8vKLOPOYYK6u6WGpZGleYZVqLREROVCs40j6hQ+ZWmRmKytfiQ6uvhQWlVJYXMYpR7XnqA7Nmb50C/l7SskrKKrS0C4iIgeKtc7mcYL5tsqAs4HJwJOJCqq+bSoI2ke6tM7m7AGd+GTlNuat3wmgEomISC1iTSRN3X0aYO6+xt3vBc5JXFj1q7KhvWvrppzdvxMlZRU8/uEqgH2DEUVEJLpYG9uLzCyNYPbfccBXQKfEhVW/9ieSbDq1akLzrHTeXbqFltkZdG2d3cDRiYg0brGWSO4kmGfrh8BQgskbb0hQTPWu8gmHnVo1oUlGOqcd3QEIqrU0el1E5OBqTSTh4MOr3H2Xu6939xvDnluf1EN89WJjfhEdWmTRJCMdgLMHBIUtNbSLiNSu1kQSdvMdakn80zwvfy9dIqqwzhnQiSYZaQztrUkZRURqE2sbyVzgVTN7geD56QC4+8sJiaqebcwvokfb/c8V6dwqm4/vPpc2TaM+s0tERCLEmkjaAduo2lPLgaRIJHkFRQzLaVdlXbvmSTWVmIhIwsSUSNz9xkQH0lD2lpSzc09plaotERGJXUyJxMweJyiBVOHuo+s8onqWV7C/66+IiBy6WKu2Xo94nw1cAWyo+3Dq38Zw+niVSEREDk+sVVsvRS6b2RTgXwmJqJ7lRYxqFxGRQ3e486P3A3rVZSANpXJUe5dWKpGIiByOWNtICqnaRpJH8IySI15efhFtmmXSNCu9oUMRETkixVq1lbRDvDfmF6k0IiISh1ifR3KFmbWOWG5jZpcnLKp6lFewVz22RETiEGsbya/dPb9ywd13Ar9OSET1LC+/iC5qaBcROWyxJpJo+8XadbjRKi4rZ+uuEpVIRETiEGsimWVm95tZXzM7ysz+CMxOZGD1YXNBMaAxJCIi8Yg1kdwOlADPAc8De4EfJCqo+hL5QCsRETk8sfba2g38LMGx1LvKUe1KJCIihy/WXltvm1mbiOW2ZvZmwqKqJ5Wj2jur+6+IyGGLtWqrQ9hTCwB330ESPLN9Y34RLZpk0DJbzx0RETlcsSaSCjPbNyWKmeUQZTbgI03Q9VelERGReMTahffnwAdm9l64fAZwS2JCqh8b8/cya80Oju/eqqFDERE5osXa2P5PM8slSB7zgFcJem4dkfL3lHLDxM8oKi3nrvP7N3Q4IiJHtFgb228CpgF3ha8ngXtjOO4CM1tqZsvN7IBeX2Gj/StmtsDMPjOz46ptTzezuWb2evVjD1dRaTljnpjJ6q17eOT6oRzXvXXtB4mISI1ibSO5AxgGrHH3s4ETgS0HO8DM0oEHgQuBQcA1Zjao2m73APPcfTAwEnggynUXxxhjrdyd26fMZfbaHdz/3SGMOLpDXZ1aRCRlxZpIity9CMDMmrj7EqC2OqHhwHJ3X+nuJcCzwGXV9hlEUNIhPGeOmXUOr9MDuBh4LMYYa7Vm2x7eXrSJO87txyWDu9XVaUVEUlqsiWR9OI7kb8DbZvYqtT9qtzuwLvIc4bpI84ErAcxsONAb6BFu+xPwE6DiYBcxs1vMbJaZzdqy5aCFJPL3lgJwvKqzRETqTEyJxN2vcPed7n4v8EtgAnB5LYdZtFNVW74PaGtm8wimYZkLlJnZJcBmd691Pi93f8Tdc909t2PHjgfdt7CoDEDjRkRE6tAhz+Dr7u/VvhcQlEB6Riz3oFopxt0LgBsBzMyAVeHrauBSM7sIyAZamdlT7n7docYbqbAoKJG0zD7iJy4WEWk0DveZ7bGYCfQzsz5mlkWQHF6L3CF8QFZWuHgTMMPdC9z9bnfv4e454XHvxJtEILJEokQiIlJXEvaN6u5lZjYOeBNIBya6+xdmNjbcPh4YCEw2s3JgETAmUfEAFOwrkahqS0SkriT0p7m7vwG8UW3d+Ij3HwP9ajnHu8C7dRFPZYmkRROVSERE6koiq7YancKiMppnpZOeFq0fgIiIHI4USySlqtYSEaljKZZIytTQLiJSx1IrkRSXKpGIiNSxlEoku4rKVLUlIlLHUiqRqGpLRKTupVQiKVCJRESkzqVUIiksKqWVSiQiInUqZRJJSVkFxWUVqtoSEaljKZNICjU9iohIQqRQItGEjSIiiZCCiUQlEhGRupRCiUTPIhERSYSUSSQFmvlXRCQhUiaRVJZIWqlqS0SkTqVQIlFju4hIIqRcImmhRCIiUqdSKJGU0jQzncz0lPnIIiL1ImW+VTVho4hIYqROItGzSEREEiJ1Eolm/hURSYgUSyQqkYiI1LUUSiSlGkMiIpIAKZRIVCIREUkEJRIREYlLSiSS0vIK9paWq7FdRCQBUiKR7NL0KCIiCZMSiaRQM/+KiCRMSiSSAj1mV0QkYVIikVSWSFqpaktEpM6lSCJRiUREJFFSJJGosV1EJFFSJJHoee0iIomSIomkskSiqi0RkbqW0ERiZheY2VIzW25mP4uyva2ZvWJmC8zsMzM7Llzf08ymm9liM/vCzO6IJ47C4jKaZKSRlZESeVNEpF4l7JvVzNKBB4ELgUHANWY2qNpu9wDz3H0wMBJ4IFxfBtzl7gOBU4AfRDk2ZoVFpSqNiIgkSCJ/og8Hlrv7SncvAZ4FLqu2zyBgGoC7LwFyzKyzu2909znh+kJgMdD9cAMpKCpT118RkQRJZCLpDqyLWF7PgclgPnAlgJkNB3oDPSJ3MLMc4ETg02gXMbNbzGyWmc3asmVL1EA0YaOISOIkMpFYlHVebfk+oK2ZzQNuB+YSVGsFJzBrAbwE3OnuBdEu4u6PuHuuu+d27NgxaiC7VLUlIpIwifyZvh7oGbHcA9gQuUOYHG4EMDMDVoUvzCyTIIk87e4vxxNIYVEZnVtlx3MKERGpQSJLJDOBfmbWx8yygKuB1yJ3MLM24TaAm4AZ7l4QJpUJwGJ3vz/eQFS1JSKSOAn7dnX3MjMbB7wJpAMT3f0LMxsbbh8PDAQmm1k5sAgYEx5+GnA9sDCs9gK4x93fOJxYCotKadFEVVsiIomQ0J/p4Rf/G9XWjY94/zHQL8pxHxC9jeWQlVc4u0vKVSIREUmQpB+hp4daiYgkVtInkspnkbRSry0RkYRI+kSimX9FRBIrBRKJnkUiIpJIKZBIVCIREUmk5E8kxXoWiYhIIiV/ItGzSEREEirpE0n+HpVIREQSKekTyYotu+jSKpvszPSGDkVEJCklfSJZkldI/y4tGzoMEZGkldSJpLS8ghVbdjFAiUREJGGSOpGs3rqb0nJXiUREJIGSOpEsySsEUCIREUmgpE4kS/MKSU8z+nZs0dChiIgkraROJEvyCslp30w9tkREEiipE8myTYUM6NKqocMQEUlqSZtIdheXsXb7HrWPiIgkWNImkmWb1NAuIlIfkjaRLA17bGkMiYhIYiVtIlmSV0izrHR6tm3W0KGIiCS1pE0kyzYV0q9zS9LSrKFDERFJakmbSJbmFdK/s8aPiIgkWlImki2FxWzbXUJ/df0VEUm4pEwkamgXEak/yZlI1PVXRKTeJGciySugQ4ssOrRo0tChiIgkvSRNJHqYlYhIfUnKB5lfdHxX2jXPaugwRERSQlImklvP7NvQIYiIpIykrNoSEZH6o0QiIiJxUSIREZG4KJGIiEhclEhERCQuSiQiIhKXhCYSM7vAzJaa2XIz+1mU7W3N7BUzW2Bmn5nZcbEeKyIijUPCEomZpQMPAhcCg4BrzGxQtd3uAea5+2BgJPDAIRwrIiKNQCJLJMOB5e6+0t1LgGeBy6rtMwiYBuDuS4AcM+sc47EiItIIJHJke3dgXcTyeuDkavvMB64EPjCz4UBvoEeMxwJgZrcAt4SLxWb2efyhJ4UOwNaGDqIR0H3YT/diP92L/frHe4JEJpJoz7j1asv3AQ+Y2TxgITAXKIvx2GCl+yPAIwBmNsvdcw834GSiexHQfdhP92I/3Yv9zGxWvOdIZCJZD/SMWO4BbIjcwd0LgBsBzMyAVeGrWW3HiohI45DINpKZQD8z62NmWcDVwGuRO5hZm3AbwE3AjDC51HqsiIg0Dgkrkbh7mZmNA94E0oGJ7v6FmY0Nt48HBgKTzawcWASMOdixMVz2kQR8lCOV7kVA92E/3Yv9dC/2i/temHvUpgcREZGYaGS7iIjERYlERETikhSJJJWnUzGznmY23cwWm9kXZnZHuL6dmb1tZl+G/7Zt6Fjri5mlm9lcM3s9XE7JexF2ZnnRzJaE/z9OTeF78aPw7+NzM5tiZtmpci/MbKKZbY4cY3ewz25md4ffpUvN7BuxXOOITySaToUy4C53HwicAvwg/Pw/A6a5ez+C2QNSKcHeASyOWE7Ve/EA8E93HwAMIbgnKXcvzKw78EMg192PI+jAczWpcy8mARdUWxf1s4ffHVcDx4bH/DX8jj2oIz6RkOLTqbj7RnefE74vJPiy6E5wD54Id3sCuLxBAqxnZtYDuBh4LGJ1yt0LM2sFnAFMAHD3EnffSQrei1AG0NTMMgjGqW0gRe6Fu88AtldbXdNnvwx41t2L3X0VsJzgO/agkiGRRJtOpXsDxdKgzCwHOBH4FOjs7hshSDZApwYMrT79CfgJUBGxLhXvxVHAFuDxsJrvMTNrTgreC3f/CvgDsBbYCOS7+1uk4L2IUNNnP6zv02RIJDFPp5LMzKwF8BJwZzioM+WY2SXAZnef3dCxNAIZwEnAQ+5+IrCb5K26Oaiw/v8yoA/QDWhuZtc1bFSN1mF9nyZDIql1KpZkZ2aZBEnkaXd/OVy9ycy6htu7ApsbKr56dBpwqZmtJqjiPMfMniI178V6YL27fxouv0iQWFLxXpwHrHL3Le5eCrwMjCA170Wlmj77YX2fJkMiSenpVMI5yiYAi939/ohNrwE3hO9vAF6t79jqm7vf7e493D2H4P/BO+5+Hal5L/KAdWZWObPruQSzR6TcvSCo0jrFzJqFfy/nErQlpuK9qFTTZ38NuNrMmphZH6Af8FltJ0uKke1mdhFB3XjldCq/bdiI6o+ZnQ68TzB7cmW7wD0E7STPA70I/pC+4+7VG9ySlpmdBfzY3S8xs/ak4L0wsxMIOh1kASsJJkhNIzXvxX8A3yXo5TiXYG6/FqTAvTCzKcBZBFPnbwJ+DfyNGj67mf0cGE1wr+5096m1XiMZEomIiDScZKjaEhGRBqREIiIicVEiERGRuCiRiIhIXJRIREQkLkokIiISFyUSkTiZ2QnhWKbK5Uvr6nEGZnanmTWri3OJJIrGkYjEycxGEUxRPi4B514dnnvrIRyT7u7ldR2LSE1UIpGUYWY54QOeHg0fcvSWmTWtYd++ZvZPM5ttZu+b2YBw/XfChyPNN7MZ4bQ8/wl818zmmdl3zWyUmf0l3H+SmT1kwcPHVprZmeGDhhab2aSI6z1kZrPCuP4jXPdDgkkGp5vZ9HDdNWa2MIzhfyKO32Vm/2lmnwKnmtl9ZrbIzBaY2R8Sc0dFQu6ul14p8QJyCKZ9OCFcfh64roZ9pwH9wvcnE8zbBcFUNN3D923Cf0cBf4k4dt8ywUOFniWYVfUyoAA4nuBH3OyIWNqF/6YD7wKDw+XVQIfwfTeC6Sw6Eszu+w5webjNgasqzwUsZX+NQ5uGvvd6JfdLJRJJNavcfV74fjZBcqkinJJ/BPCCmc0DHga6hps/BCaZ2c0EX/qx+Lu7O0ES2uTuC929Avgi4vpXmdkcgnmgjiV42md1w4B3PZjFtgx4muDhVQDlBDNAQ5CsioDHzOxKYE+McYocloyGDkCknhVHvC8HolVtpQE73f2E6hvcfayZnUzwFMZ54cSIsV6zotr1K4CMcJbVHwPD3H1HWOWVHeU80Z4VUanIw3YRdy8zs+EEs9xeDYwDzokhTpHDohKJSDUePBhslZl9B4Kp+s1sSPi+r7t/6u6/ArYSPLuhEGgZxyVbETx4Kt/MOgMXRmyLPPenwJlm1iF8jvY1wHvVTxaWqFq7+xvAncAJccQmUiuVSESiuxZ4yMx+AWQStHPMB35vZv0ISgfTwnVrgZ+F1WC/O9QLuft8M5tLUNW1kqD6rNIjwFQz2+juZ5vZ3cD08PpvuHu0Z2i0BF41s+xwvx8dakwih0Ldf0VEJC6q2hIRkbioaktSmpk9SPCs90gPuPvjDRGPyJFIVVsiIhIXVW2JiEhclEhERCQuSiQiIhIXJRIREYnL/wc3l9nws1rcwgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "n_estimators = list(range(1,101))\n",
    "\n",
    "#collect max datapoints\n",
    "max_scores = []\n",
    "scores = gs2.cv_results_['mean_test_score'].round(4)\n",
    "max_score = np.max(scores)\n",
    "\n",
    "max_point = [m for m in range(len(scores)) if scores[m] == max_score]\n",
    "\n",
    "#Graph Info\n",
    "plt.plot(n_estimators, scores)\n",
    "\n",
    "plt.title('Elbow Graph')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('accuracy')\n",
    "\n",
    "plt.xlim(0,100)\n",
    "plt.ylim(0.9,1.0)\n",
    "\n",
    "plt.plot([max_point],[max_score], marker = 'o', markersize = 10, markerfacecolor = None, markeredgecolor = 'r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**code for the maximum point to be labelled**\n",
    "\n",
    "    if len(max_point) == 1 :\n",
    "        max_text='max point ['+ str(max_point[0])+','+str(max_score)+']'\n",
    "        plt.annotate(text=max_text,xy=(max_point[0],max_score),xytext=(max_point[0]-10,max_score+0.01),color='r',size=10)\n",
    "    else:\n",
    "        max_text='max : '+str(max_score)+' ('+str(len(max_scores))+' points)'\n",
    "        plt.text(30.0,0.91,max_text,c='b',size =10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at this graph, we see that around 10 trees the graph levels out. The best model occurred at n_estimators=29 but given how volatile it is, that was probably due to random chance. We choose about 10 to be our number of estimators, because we want the minimum number of estimators that still yield maximum performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build our random forest model with the optimal number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=10)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rffinal = RandomForestClassifier(n_estimators = 10)\n",
    "rffinal.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elbow graphs pop up in lots of different situations when we are adding complexity to a model and want to determine the minimal amount of complexity that will yield optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does every feature contribute equally to building a model?\n",
    "\n",
    "If not, which subset of features should we use? \n",
    "\n",
    "This is a matter of **feature selection.**\n",
    "\n",
    "Random forests provide a straightforward method for feature selection: **mean decrease impurity.** \n",
    "\n",
    "A random forest consists of many decision trees, and that for each tree, the node is chosen to split the dataset based on maximum decrease in impurity, typically either Gini impurity or entropy in classification. \n",
    "\n",
    "Thus for a tree, it can be computed how much impurity each feature decreases in a tree, and then for a forest, the impurity decrease from each feature can be averaged. Consider this measure a metric of importance of each feature, we then can rank and select the features according to feature importance.\n",
    "\n",
    "Scikit-learn provides a feature_importances_ variable with the model, which shows the relative importance of each feature. The scores are scaled down so that the sum of all scores is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "worst radius            0.309701\n",
       "mean concave points     0.183126\n",
       "worst concave points    0.115641\n",
       "mean perimeter          0.064119\n",
       "mean radius             0.058742\n",
       "worst concavity         0.050951\n",
       "radius error            0.049103\n",
       "mean texture            0.017197\n",
       "worst area              0.016512\n",
       "mean concavity          0.014696\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf3 = RandomForestClassifier(n_estimators = 10, random_state = 111)\n",
    "\n",
    "rf3.fit(X_train, y_train)\n",
    "\n",
    "ft_imp = pd.Series(rf3.feature_importances_, index = cancer_data.feature_names).sort_values(ascending = False)\n",
    "\n",
    "ft_imp.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In above code)We find the feature importances in a random forest with n_estimator = 10 using the training dataset, and display them in the descending order.\n",
    "\n",
    "From the output, we can see that among all features, worst radius is most important (0.31), followed by mean concave points and worst concave points.\n",
    "\n",
    "In regression, we calculate the feature importance using variance instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why should we perform feature selection? Top reasons are:\n",
    "\n",
    "    it enables us to train a model faster;\n",
    "    it reduces the complexity of a model thus makes it easier to interpret.\n",
    "    And if the right subset is chosen, it can improve the accuracy of a model.\n",
    "    Choosing the right subset often relies on domain knowledge, some art, and a bit of luck.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  can try to build a model with higher importances, and see if it improves accuracy. \n",
    "\n",
    "The advantage of building a model using less features is more pronounced when the sample size is large.\n",
    "\n",
    "This is because of removal of some noise(features) in the model.\n",
    "\n",
    "Grabbing columns by name:\n",
    "\n",
    "     worst_cols = [col for col in df.columns if 'worst' in col]\n",
    "     X_worst = df[worst_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Pros & Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance**\n",
    "\n",
    "Probably the biggest advantage of Random Forests is that they generally perform well without any tuning. They will also perform decently well on almost every dataset.\n",
    "\n",
    "A linear model, for example, cannot perform well on a dataset that cannot be split with a line. It is not possible to split the following dataset with a line without manipulating the features. However, a random forest will perform just fine on this dataset.\n",
    "\n",
    "We can see this by looking at the code below to generate the fake dataset and comparing a Logistic Regression model with a Random Forest model.\n",
    "    \n",
    "    The function make_circles makes a classification dataset with concentric circles. \n",
    "    We use kfold cross validation to compare the accuracy scores and see that the Logistic Regression model performs worse \n",
    "    than random guessing but the Random Forest model performs quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR accuracy: 0.36\n",
      "RF accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X, y = make_circles(noise=0.2, factor=0.5, random_state=1)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "lr_scores = []\n",
    "rf_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    lr = LogisticRegression(solver='lbfgs')\n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_scores.append(lr.score(X_test, y_test))\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100)\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_scores.append(rf.score(X_test, y_test))\n",
    "    \n",
    "print(\"LR accuracy:\", np.mean(lr_scores))\n",
    "print(\"RF accuracy:\", np.mean(rf_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking to get a benchmark for a classification problem; \n",
    "     \n",
    "     it is common practice to start by building a Logistic Regression model and a Random Forest model as these two models \n",
    "     both have potential to perform well without any tuning.\n",
    "     This will give you values for your metrics to try to beat. Often times it is almost impossible to do better than these \n",
    "     benchmarks.\n",
    "     \n",
    "\n",
    "solver{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’ Algorithm to use in the optimization problem.\n",
    "\n",
    "    LogisticRegression(solver='lbfgs') \n",
    "\n",
    "    1) For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones. \n",
    "    2) For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss; \n",
    "    3) ‘liblinear’ is limited to one-versus-rest schemes. ‘newton-cg’, ‘lbfgs’, ‘sag’ and ‘saga’ handle L2 or no penalty \n",
    "    4) ‘liblinear’ and ‘saga’ also handle L1 penalty ‘saga’ also supports ‘elasticnet’ penalty ‘liblinear’ does not support \n",
    "        setting penalty='none'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretability**\n",
    "\n",
    "Random Forests, despite being made up of Decision Trees, are not easy to interpret. A random forest has several decision trees, each of which is not a very good model, but when averaged, create an excellent model. Thus Random Forests are not a good choice when looking for interpretability.\n",
    "\n",
    "In most cases, interpretability is not important.\n",
    "\n",
    "**Computation**\n",
    "\n",
    "Random Forests can be a little slow to build, especially if you have a lot of trees in the random forest. Building a random forest involves building 10-100 (usually) decision trees. Each of the decision trees is faster to build than a standard decision tree because of how we do not compare every feature at every split, however given the quantity of decision trees it is often slow to build.\n",
    "\n",
    "Similarly, predicting with a Random Forest will be slower than a Decision Tree since we have to do a prediction with each of the 10-100 decision trees in order to get our final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
